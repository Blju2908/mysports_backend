"""
LLM-based agents for prompt refinement using Langchain.
"""

import json
import asyncio
from typing import Optional, Dict, Any, Tuple
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

from app.core.config import get_config
from .schemas import CritiqueResponse, ValidationResponse, ValidationDecision, RefinedPrompts
from .prompts import (
    CRITIQUE_AGENT_PROMPT,
    REFINE_AGENT_PROMPT,
    VALIDATE_AGENT_PROMPT,
    VALUE_PROPOSITION_TEXT,
)


class PromptRefinementAgents:
    """
    Collection of LLM-based agents for prompt refinement.
    Each agent has a specific role: critique, refine, or validate.
    """

    def __init__(self, provider: str = "openai", model_name: str = "gpt-4o"):
        self.provider = provider
        self.model_name = model_name
        self.llm = self._create_llm()

    def _create_llm(self):
        """Create the LLM instance based on provider and model"""
        if self.provider == "openai":
            return ChatOpenAI(
                model=self.model_name,
                api_key=get_config().OPENAI_API_KEY2,
            )
        elif self.provider == "anthropic":
            return ChatAnthropic(
                model=self.model_name,
                api_key=get_config().ANTHROPIC_API_KEY,
                temperature=0.1,
            )
        elif self.provider == "google":
            return ChatGoogleGenerativeAI(
                model=self.model_name,
                google_api_key=get_config().GOOGLE_API_KEY,
                temperature=0.1,
            )
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    async def critique_workout(
        self,
        system_prompt: str,
        user_data: str,
        generated_workout: str,
    ) -> CritiqueResponse:
        """
        Critique Agent: Analyzes the workout and identifies weaknesses.
        
        Args:
            system_prompt: The system prompt used for workout generation
            user_data: User training plan and history data
            generated_workout: The workout generated by the LLM
            
        Returns:
            CritiqueResponse: Structured critique with scores and improvements
        """
        formatted_prompt = CRITIQUE_AGENT_PROMPT.format(
            value_proposition=VALUE_PROPOSITION_TEXT,
            system_prompt=system_prompt,
            user_data=user_data,
            generated_workout=generated_workout,
        )

        # Use structured output with Pydantic model
        structured_llm = self.llm.with_structured_output(CritiqueResponse)
        
        print("ðŸ” Critique Agent: Analyzing workout quality...")
        start_time = datetime.now()
        
        try:
            critique = await structured_llm.ainvoke(formatted_prompt)
            duration = (datetime.now() - start_time).total_seconds()
            print(f"âœ… Critique completed in {duration:.1f}s (Score: {critique.overall_score}/10)")
            return critique
        except Exception as e:
            print(f"âŒ Critique Agent failed: {e}")
            # Return minimal fallback critique
            return CritiqueResponse(
                overall_score=5,
                strengths=["Fallback response due to agent failure"],
                weaknesses=[],
                key_improvements=["Agent failure occurred - check logs"]
            )

    async def refine_prompt(
        self,
        prompt_template: str,
        training_principles: str,
        critique: CritiqueResponse,
    ) -> RefinedPrompts:
        """
        Refine Agent: Improves both the prompt template and training principles.
        
        Args:
            prompt_template: The original prompt template
            training_principles: The original training principles
            critique: The critique response from the critique agent
            
        Returns:
            RefinedPrompts: A Pydantic object with the refined prompts
        """
        formatted_prompt = REFINE_AGENT_PROMPT.format(
            prompt_template=prompt_template,
            training_principles=training_principles,
            critique_json=critique.model_dump_json(indent=2),
        )

        structured_llm = self.llm.with_structured_output(RefinedPrompts)

        print("âš¡ Refine Agent: Optimizing system prompts...")
        start_time = datetime.now()
        
        try:
            refined_prompts = await structured_llm.ainvoke(formatted_prompt)
            duration = (datetime.now() - start_time).total_seconds()
            print(f"âœ… Prompts refined in {duration:.1f}s")
            return refined_prompts
        except Exception as e:
            print(f"âŒ Refine Agent failed: {e}")
            # Return original prompts as fallback
            return RefinedPrompts(
                prompt_template=prompt_template,
                training_principles=training_principles
            )

    async def validate_workouts(
        self,
        user_data: str,
        workout_v1: str,
        workout_v2: str,
    ) -> ValidationResponse:
        """
        Validate Agent: Compares two workouts and determines which is better.
        
        Args:
            user_data: User training plan and history data
            workout_v1: The original workout
            workout_v2: The improved workout
            
        Returns:
            ValidationResponse: Decision on which workout is better
        """
        formatted_prompt = VALIDATE_AGENT_PROMPT.format(
            user_data=user_data,
            workout_v1=workout_v1,
            workout_v2=workout_v2,
        )

        print("âš–ï¸ Validate Agent: Comparing workouts...")
        start_time = datetime.now()
        
        try:
            # Use regular text generation and parse the result
            response = await self.llm.ainvoke(formatted_prompt)
            content = response.content.strip()
            
            # Parse the decision from the last line
            lines = content.split('\n')
            decision_line = lines[-1].strip()
            
            # Extract reasoning (everything except the last line)
            reasoning = '\n'.join(lines[:-1]).strip()
            
            # Validate decision
            if decision_line in ["WORKOUT_1", "WORKOUT_2", "TIE"]:
                decision = ValidationDecision(decision_line)
            else:
                print(f"âš ï¸ Invalid decision format: {decision_line}, defaulting to TIE")
                decision = ValidationDecision.TIE
            
            duration = (datetime.now() - start_time).total_seconds()
            print(f"âœ… Validation completed in {duration:.1f}s (Decision: {decision.value})")
            
            return ValidationResponse(
                reasoning=reasoning,
                decision=decision
            )
            
        except Exception as e:
            print(f"âŒ Validate Agent failed: {e}")
            # Return fallback validation
            return ValidationResponse(
                reasoning="Validation failed due to agent error. Defaulting to no improvement.",
                decision=ValidationDecision.WORKOUT_1
            )

    async def run_all_agents(
        self,
        system_prompt: str,
        user_data: str,
        workout_v1: str,
        workout_v2: str,
    ) -> Tuple[CritiqueResponse, str, ValidationResponse]:
        """
        Run all three agents in sequence.
        
        Args:
            system_prompt: Original system prompt
            user_data: User data for context
            workout_v1: Original workout
            workout_v2: Improved workout
            
        Returns:
            Tuple of (critique, refined_prompt, validation)
        """
        print("ðŸš€ Running all refinement agents...")
        start_time = datetime.now()
        
        # Run critique and refine in parallel (they don't depend on each other)
        critique_task = self.critique_workout(system_prompt, user_data, workout_v1)
        
        # Wait for critique to complete before refining
        critique = await critique_task
        
        # Run refine and validate in parallel
        refine_task = self.refine_prompt(system_prompt, user_data, critique)
        validate_task = self.validate_workouts(user_data, workout_v1, workout_v2)
        
        refined_prompt, validation = await asyncio.gather(
            refine_task,
            validate_task
        )
        
        total_duration = (datetime.now() - start_time).total_seconds()
        print(f"âœ… All agents completed in {total_duration:.1f}s")
        
        return critique, refined_prompt, validation 